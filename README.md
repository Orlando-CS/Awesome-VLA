# 🌟 Top Vision-Language-Action (VLA) Models

🔥🔥🔥 **RT-2: Robotics Transformer 2 — End-to-End Vision-Language-Action Models**  
<p align="center">
    <img src="./images/rt2.jpg" width="60%" height="60%">
</p>

<div align='center'> [📖 Project Page](https://robotics-transformer2.github.io/) | [📄 Paper](https://arxiv.org/abs/2307.15818) </div>

<div align='center'>Integrates vision-language models trained on internet-scale data directly into robotic control pipelines. ✨</div>

---

🔥🔥🔥 **Helix: Generalist VLA Model for Full-Body Humanoid Control**  
<p align="center">
    <img src="./images/helix.jpg" width="60%" height="60%">
</p>

<div align='center'> [📖 Helix Announcement](https://www.figure.ai/news/helix) </div>

<div align='center'>The first VLA model to enable full upper-body humanoid control including fingers, wrists, torso, and head. ✨</div>

---

🔥🔥🔥 **π0 (Pi-Zero): Generalist VLA Across Diverse Robots**  

<div align='center'> [📖 Hugging Face Blog](https://huggingface.co/blog/pi0) </div>

<div align='center'>Designed for dexterous manipulation across different robot types with flow matching-based action generation. ✨</div>

---

🔥🔥🔥 **OpenVLA: Open-Source Large-Scale Vision-Language-Action Model**  

<div align='center'> [📖 OpenVLA Project Page](https://openvla.github.io/) </div>

<div align='center'>Pretrained on 970K+ robot episodes, sets a new benchmark for open generalist robot policies. ✨</div>

---

🔥🔥🔥 **Gemini Robotics: Multimodal Generalization to Physical Action**  
<p align="center">
    <img src="./images/gemini_robotics.webp" width="60%" height="60%">
</p>

<div align='center'> [📖 The Verge Report](https://www.theverge.com/news/628021/google-deepmind-gemini-robotics-ai-models) </div>

<div align='center'>Built on Gemini 2.0, enabling complex, real-world manipulation without task-specific training. ✨</div>

---

# 📋 Table Overview of Top Vision-Language-Action (VLA) Models

| Model | Organization | Key Contribution | Paper | Project |
|:---|:---:|:---|:---:|:---:|
| **RT-2** | Google DeepMind | Co-fine-tunes on robot data + internet-scale VLM data for semantic robotic reasoning. | [📄 arXiv](https://arxiv.org/abs/2307.15818) | [🌐 Project](https://robotics-transformer2.github.io/) |
| **Helix** | Figure AI | First model achieving full humanoid body (finger+torso+head) high-rate control. | - | [🌐 Announcement](https://www.figure.ai/news/helix) |
| **π0 (Pi-Zero)** | Physical Intelligence | Generalist action generation across diverse robot embodiments using flow matching. | - | [🌐 Hugging Face Blog](https://huggingface.co/blog/pi0) |
| **OpenVLA** | Stanford | Open-source 7B VLA model trained on 970k+ robot episodes for multi-robot adaptation. | - | [🌐 Project](https://openvla.github.io/) |
| **Gemini Robotics** | Google DeepMind | Multimodal reasoning without task-specific training, enabling dexterous physical interaction. | - | [🌐 Verge Article](https://www.theverge.com/news/628021/google-deepmind-gemini-robotics-ai-models) |

---

# 📑 Table of Contents

- [🔥 Top Vision-Language-Action (VLA) Models](#-top-vision-language-action-vla-models)
- [📋 Table Overview of Top Vision-Language-Action (VLA) Models](#-table-overview-of-top-vision-language-action-vla-models)
