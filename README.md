# ğŸŒŸ Top Vision-Language-Action (VLA) Models

ğŸ”¥ğŸ”¥ğŸ”¥ **RT-2: Robotics Transformer 2 â€” End-to-End Vision-Language-Action Models**  
<p align="center">
    <img src="./images/rt2.jpg" width="60%" height="60%">
</p>

<div align='center'> [ğŸ“– Project Page](https://robotics-transformer2.github.io/) | [ğŸ“„ Paper](https://arxiv.org/abs/2307.15818) </div>

<div align='center'>Integrates vision-language models trained on internet-scale data directly into robotic control pipelines. âœ¨</div>

---

ğŸ”¥ğŸ”¥ğŸ”¥ **Helix: Generalist VLA Model for Full-Body Humanoid Control**  
<p align="center">
    <img src="./images/helix.jpg" width="60%" height="60%">
</p>

<div align='center'> [ğŸ“– Helix Announcement](https://www.figure.ai/news/helix) </div>

<div align='center'>The first VLA model to enable full upper-body humanoid control including fingers, wrists, torso, and head. âœ¨</div>

---

ğŸ”¥ğŸ”¥ğŸ”¥ **Ï€0 (Pi-Zero): Generalist VLA Across Diverse Robots**  
<p align="center">
    <img src="./images/pizero.jpg" width="60%" height="60%">
</p>

<div align='center'> [ğŸ“– Hugging Face Blog](https://huggingface.co/blog/pi0) </div>

<div align='center'>Designed for dexterous manipulation across different robot types with flow matching-based action generation. âœ¨</div>

---

ğŸ”¥ğŸ”¥ğŸ”¥ **OpenVLA: Open-Source Large-Scale Vision-Language-Action Model**  
<p align="center">
    <img src="./images/openvla.jpg" width="60%" height="60%">
</p>

<div align='center'> [ğŸ“– OpenVLA Project Page](https://openvla.github.io/) </div>

<div align='center'>Pretrained on 970K+ robot episodes, sets a new benchmark for open generalist robot policies. âœ¨</div>

---

ğŸ”¥ğŸ”¥ğŸ”¥ **Gemini Robotics: Multimodal Generalization to Physical Action**  
<p align="center">
    <img src="./images/gemini_robotics.jpg" width="60%" height="60%">
</p>

<div align='center'> [ğŸ“– The Verge Report](https://www.theverge.com/news/628021/google-deepmind-gemini-robotics-ai-models) </div>

<div align='center'>Built on Gemini 2.0, enabling complex, real-world manipulation without task-specific training. âœ¨</div>

---

# ğŸ“‹ Table Overview of Top Vision-Language-Action (VLA) Models

| Model | Organization | Key Contribution | Paper | Project |
|:---|:---:|:---|:---:|:---:|
| **RT-2** | Google DeepMind | Co-fine-tunes on robot data + internet-scale VLM data for semantic robotic reasoning. | [ğŸ“„ arXiv](https://arxiv.org/abs/2307.15818) | [ğŸŒ Project](https://robotics-transformer2.github.io/) |
| **Helix** | Figure AI | First model achieving full humanoid body (finger+torso+head) high-rate control. | - | [ğŸŒ Announcement](https://www.figure.ai/news/helix) |
| **Ï€0 (Pi-Zero)** | Physical Intelligence | Generalist action generation across diverse robot embodiments using flow matching. | - | [ğŸŒ Hugging Face Blog](https://huggingface.co/blog/pi0) |
| **OpenVLA** | Stanford | Open-source 7B VLA model trained on 970k+ robot episodes for multi-robot adaptation. | - | [ğŸŒ Project](https://openvla.github.io/) |
| **Gemini Robotics** | Google DeepMind | Multimodal reasoning without task-specific training, enabling dexterous physical interaction. | - | [ğŸŒ Verge Article](https://www.theverge.com/news/628021/google-deepmind-gemini-robotics-ai-models) |

---

# ğŸ“‘ Table of Contents

- [ğŸ”¥ Top Vision-Language-Action (VLA) Models](#-top-vision-language-action-vla-models)
- [ğŸ“‹ Table Overview of Top Vision-Language-Action (VLA) Models](#-table-overview-of-top-vision-language-action-vla-models)
