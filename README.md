# Awesome VLA Models

ðŸ”¥ðŸ”¥ðŸ”¥ **RT-2: Robotics Transformer 2 â€” End-to-End Vision-Language-Action Model**  
<p align="center">
    <img src="./images/rt2.jpg" width="60%" height="60%">
</p>

<font size=7><div align='center'>[[ðŸ“– Paper](https://arxiv.org/abs/2307.15818)] [[ðŸŒŸ Project](https://robotics-transformer2.github.io/)]</div></font>  

<font size=7><div align='center'>Integrates vision-language models trained on internet-scale data directly into robotic control pipelines. âœ¨</div></font>

---

ðŸ”¥ðŸ”¥ðŸ”¥ **Helix: Generalist VLA Model for Full-Body Humanoid Control**  
<p align="center">
    <img src="./images/helix.jpg" width="60%" height="60%">
</p>

<font size=7><div align='center'>[[ðŸŒŸ Project](https://www.figure.ai/news/helix)]</div></font>  

<font size=7><div align='center'>First VLA model achieving full upper-body humanoid control including fingers, wrists, torso, and head. âœ¨</div></font>

---

ðŸ”¥ðŸ”¥ðŸ”¥ **Ï€0 (Pi-Zero): Generalist VLA Across Diverse Robots**  

<font size=7><div align='center'>[[ðŸŒŸ Project](https://huggingface.co/blog/pi0)]</div></font>  

<font size=7><div align='center'>Generalist control across various robot embodiments, utilizing large-scale pretraining and flow matching action generation. âœ¨</div></font>

---

ðŸ”¥ðŸ”¥ðŸ”¥ **OpenVLA: Open-Source Large-Scale Vision-Language-Action Model**  

<font size=7><div align='center'>[[ðŸ“– Paper](https://arxiv.org/abs/2406.09246)] [[ðŸŒŸ Project](https://openvla.github.io/)] [[ðŸ¤– Hugging Face](https://huggingface.co/openvla/openvla-7b)]</div></font>  

<font size=7><div align='center'>Pretrained on 970k+ robotic episodes, setting a new benchmark for generalist robotic policies. âœ¨</div></font>

---

ðŸ”¥ðŸ”¥ðŸ”¥ **Gemini Robotics: Multimodal Generalization to Physical Action**  
<p align="center">
    <img src="./images/gemini_robotics.webp" width="60%" height="60%">
</p>

<font size=7><div align='center'>[[ðŸŒŸ Project](https://www.theverge.com/news/628021/google-deepmind-gemini-robotics-ai-models)]</div></font>  

<font size=7><div align='center'>Built on Gemini 2.0, enabling complex real-world manipulation without task-specific training. âœ¨</div></font>

---

# ðŸ“‹ Awesome Papers

| Model | Organization | Key Contribution | ðŸ“– Paper | ðŸŒŸ Project | ðŸ¤– Hugging Face |
|:---|:---:|:---|:---:|:---:|:---:|
| **RT-2** | Google DeepMind | Co-fine-tunes on robot data and internet-scale VLM data for semantic robotic reasoning. | [Paper](https://arxiv.org/abs/2307.15818) | [Project](https://robotics-transformer2.github.io/) | - |
| **Helix** | Figure AI | First model achieving full upper-body humanoid control. | - | [Project](https://www.figure.ai/news/helix) | - |
| **Ï€0 (Pi-Zero)** | Physical Intelligence | Flow matching action generation across robots. | - | [Project](https://huggingface.co/blog/pi0) | - |
| **OpenVLA** | Stanford | Open-sourced 7B model trained on 970k+ robotic episodes. | [Paper](https://arxiv.org/abs/2406.09246) | [Project](https://openvla.github.io/) | [Hugging Face](https://huggingface.co/openvla/openvla-7b) |
| **Gemini Robotics** | Google DeepMind | Real-world physical actions without task-specific training. | - | [Project](https://www.theverge.com/news/628021/google-deepmind-gemini-robotics-ai-models) | - |

---

# ðŸ“‘ Table of Contents

- [Awesome VLA Models](#awesome-vla-models)
- [Awesome Papers](#awesome-papers)
