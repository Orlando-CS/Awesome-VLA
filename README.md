<div align="center">
    <h1>Awesome Vison-Language-Action Models</h1>
    <a href="https://awesome.re"><img src="https://awesome.re/badge.svg"/></a>
    <img src=https://img.shields.io/github/stars/Orlando-CS/Awesome-VLA.svg?style=social >
</div>

This is a collection of research papers about Embodied Multimodal Large Language Models (VLA models).

If you would like to include your paper or update any details (e.g., code urls, conference information), please feel free to submit a pull request or email me. Any advice is also welcome.

# ðŸ“‘ Table of Contents

- [Awesome VLA Models](#awesome-vla-models)
- [Awesome Papers](#awesome-papers)
  

# Awesome VLA Models
ðŸ”¥ðŸ”¥ðŸ”¥ **RT-2: Robotics Transformer 2 â€” End-to-End Vision-Language-Action Model**  
<p align="center">
    <img src="./images/rt2.jpg" width="60%" height="60%">
</p>

<font size=7><div align='center'>[[ðŸ“– Paper](https://arxiv.org/abs/2307.15818)] [[ðŸŒŸ Project](https://robotics-transformer2.github.io/)]</div></font>  

<font size=7><div align='center'>Integrates vision-language models trained on internet-scale data directly into robotic control pipelines. âœ¨</div></font>

---

ðŸ”¥ðŸ”¥ðŸ”¥ **Helix: Generalist VLA Model for Full-Body Humanoid Control**  
<p align="center">
    <img src="./images/helix.jpg" width="60%" height="60%">
</p>

<font size=7><div align='center'>[[ðŸŒŸ Project](https://www.figure.ai/news/helix)]</div></font>  

<font size=7><div align='center'>First VLA model achieving full upper-body humanoid control including fingers, wrists, torso, and head. âœ¨</div></font>

---

ðŸ”¥ðŸ”¥ðŸ”¥ **Ï€0 (Pi-Zero): Generalist VLA Across Diverse Robots**  

<font size=7><div align='center'>[[ðŸŒŸ Project](https://huggingface.co/blog/pi0)]</div></font>  

<font size=7><div align='center'>Generalist control across various robot embodiments, utilizing large-scale pretraining and flow matching action generation. âœ¨</div></font>

---

ðŸ”¥ðŸ”¥ðŸ”¥ **OpenVLA: Open-Source Large-Scale Vision-Language-Action Model**  

<font size=7><div align='center'>[[ðŸ“– Paper](https://arxiv.org/abs/2406.09246)] [[ðŸŒŸ Project](https://openvla.github.io/)] [[ðŸ¤– Hugging Face](https://huggingface.co/openvla/openvla-7b)]</div></font>  

<font size=7><div align='center'>Pretrained on 970k+ robotic episodes, setting a new benchmark for generalist robotic policies. âœ¨</div></font>

---

ðŸ”¥ðŸ”¥ðŸ”¥ **Gemini Robotics: Multimodal Generalization to Physical Action**  
<p align="center">
    <img src="./images/gemini_robotics.webp" width="60%" height="60%">
</p>

<font size=7><div align='center'>[[ðŸŒŸ Project](https://www.theverge.com/news/628021/google-deepmind-gemini-robotics-ai-models)]</div></font>  

<font size=7><div align='center'>Built on Gemini 2.0, enabling complex real-world manipulation without task-specific training. âœ¨</div></font>

---

# Awesome Papers

| Model | Organization | Key Contribution | Project/Hugging Face | Time |
|:---|:---:|:---|:---:|:---:|
| [**RT-2**](https://robotics-transformer2.github.io/) | Google DeepMind | Co-fine-tunes on robot data and internet-scale VLM data for semantic robotic reasoning. | [Project](https://robotics-transformer2.github.io/) | 2023-07 |
| [**Helix**](https://www.figure.ai/news/helix) | Figure AI | First model achieving full upper-body humanoid control. | [Project](https://www.figure.ai/news/helix) | 2024-04 |
| [**Ï€0 (Pi-Zero)**](https://huggingface.co/blog/pi0) | Physical Intelligence | Flow matching action generation across robots. | [Project](https://huggingface.co/blog/pi0) | 2024-04 |
| [**OpenVLA**](https://openvla.github.io/) | Stanford | Open-sourced 7B model trained on 970k+ robotic episodes. | [Project](https://openvla.github.io/) | 2024-06 |
| [**Gemini Robotics**](https://www.theverge.com/news/628021/google-deepmind-gemini-robotics-ai-models) | Google DeepMind | Real-world physical actions without task-specific training. | [Project](https://www.theverge.com/news/628021/google-deepmind-gemini-robotics-ai-models) | 2024-05 |
| [**TinyVLA**](https://arxiv.org/abs/2409.12514) | - | Compact VLA models with faster inference speeds and improved data efficiency | [Paper](https://arxiv.org/abs/2409.12514) | 2024-09 |
| [**VLA Model-Expert Collaboration**](https://arxiv.org/abs/2503.04163) | - | Enhances VLA model performance through collaboration with a limited number of expert actions | [Paper](https://arxiv.org/abs/2503.04163) | 2025-03 |

---


